{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a11ae3b-9995-4a7f-8d2a-45b247721cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. What is the KNN algorithm?\n",
    "\n",
    "#Ans\n",
    "\n",
    "#The K-Nearest Neighbors (KNN) algorithm is a supervised machine learning algorithm used for both classification and regression tasks. It operates on the principle that similar data points are more likely to belong to the same class or have similar output values. Given a new input, KNN finds the K nearest data points in the training set based on a distance metric (e.g., Euclidean distance) and assigns a class or predicts a value based on the majority vote or average of the K neighbors, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc9c42a2-6ccc-4003-851c-553bbe818576",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. How do you choose the value of K in KNN?\n",
    "\n",
    "#Ans\n",
    "\n",
    "#The choice of K in KNN is crucial and depends on the dataset and problem at hand. A smaller value of K (e.g., 1) can lead to a more flexible decision boundary but may be sensitive to noise. On the other hand, a larger value of K reduces noise sensitivity but may smooth out important patterns. The value of K is typically chosen through experimentation or using techniques like cross-validation, where different values of K are tested and the performance is evaluated using a suitable metric (e.g., accuracy or mean squared error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "975f48dd-0040-4ab3-884a-897517304a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. What is the difference between KNN classifier and KNN regressor?\n",
    "\n",
    "#Ans\n",
    "\n",
    "#The main difference between the KNN classifier and KNN regressor lies in their output. The KNN classifier predicts the class or category of a new data point based on the majority vote of its K nearest neighbors. In contrast, the KNN regressor predicts a continuous value or quantity by averaging the output values of its K nearest neighbors. The classifier outputs discrete class labels, while the regressor outputs continuous values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6deaef11-4c19-4698-935b-76dffd7059b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4. How do you measure the performance of KNN?\n",
    "\n",
    "#Ans\n",
    "\n",
    "#The performance of KNN can be measured using different evaluation metrics depending on the task. For classification, common metrics include accuracy, precision, recall, F1-score, and area under the receiver operating characteristic curve (AUC-ROC). For regression, common metrics include mean squared error (MSE), mean absolute error (MAE), and R-squared. The choice of metric depends on the specific problem and the evaluation criteria that are important for the application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a502f88-91be-400b-8028-c500124cfdeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5. What is the curse of dimensionality in KNN?\n",
    "\n",
    "#Ans\n",
    "\n",
    "#The curse of dimensionality refers to the challenge that arises when working with high-dimensional data in KNN and other machine learning algorithms. As the number of dimensions (features) increases, the data becomes increasingly sparse in the feature space. This sparsity causes distances between points to become less meaningful, making it difficult to find meaningful nearest neighbors. The curse of dimensionality can lead to decreased performance and increased computational complexity in KNN and other algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a1c58f7-fb81-4c93-8276-447a669bd26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#6. How do you handle missing values in KNN?\n",
    "\n",
    "#Ans\n",
    "\n",
    "#Handling missing values in KNN can be done by imputing the missing values with estimated values based on the available data. One common approach is to replace the missing values with the mean or median of the respective feature. Another approach is to use the KNN algorithm itself to estimate the missing values. In this case, the missing values are treated as separate entities during the distance calculation and neighbor selection process, and their values are estimated based on the values of their K nearest neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc1344bb-d381-4cbd-b315-31f9cf7f21db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#7. Compare and contrast the performance of the KNN classifier and regressor. Which one is better for which type of problem?\n",
    "\n",
    "#Ans\n",
    "\n",
    "#The performance of the KNN classifier and regressor depends on the specific problem and the nature of the data. The KNN classifier is suitable for problems where the task is to assign discrete class labels to new instances based on their nearest neighbors. It works well when the decision boundaries are relatively simple and the classes are well-separated. The KNN regressor, on the other hand, is better suited for problems where the task is to predict continuous values or quantities. It is effective when there are underlying patterns in the data that can be captured by averaging the values of nearby instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59fd75e2-5bdb-4346-815d-475663f31951",
   "metadata": {},
   "outputs": [],
   "source": [
    "#8. What are the strengths and weaknesses of the KNN algorithm for classification and regression tasks, and how can these be addressed?\n",
    "\n",
    "#Ans\n",
    "\n",
    "#The strengths of the KNN algorithm include its simplicity, ease of implementation, and ability to handle both classification and regression tasks. KNN can capture complex relationships in the data and is non-parametric, meaning it does not make strong assumptions about the data distribution. However, KNN has weaknesses such as computational inefficiency, especially with large datasets, and sensitivity to irrelevant or noisy features. These weaknesses can be addressed by techniques such as feature selection or dimensionality reduction to reduce the number of features, using efficient data structures like KD-trees for faster nearest neighbor searches, or employing distance weighting schemes to give more importance to closer neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d38d4390-394f-4c5d-91a5-99596ba8b466",
   "metadata": {},
   "outputs": [],
   "source": [
    "#9. What is the difference between Euclidean distance and Manhattan distance in KNN?\n",
    "\n",
    "#Ans\n",
    "\n",
    "#Euclidean distance and Manhattan distance are two common distance metrics used in KNN to measure the similarity between data points. Euclidean distance is the straight-line distance between two points in the Euclidean space, calculated as the square root of the sum of squared differences between the coordinates of the points. It considers both the magnitude and direction of the differences. In contrast, Manhattan distance, also known as city block distance or L1 distance, is the sum of absolute differences between the coordinates of two points. It measures the distance based on the sum of vertical and horizontal movements required to reach one point from the other. Euclidean distance is sensitive to the scale of the features, while Manhattan distance is not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c419881f-34dd-4cce-976f-0a50438b1d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#10. What is the role of feature scaling in KNN?\n",
    "\n",
    "#Ans\n",
    "\n",
    "#Feature scaling plays an important role in KNN as it helps to ensure that all features contribute equally to the distance calculation. Since KNN relies on distance metrics, features with larger scales can dominate the distance calculations compared to features with smaller scales. Therefore, it is necessary to scale the features to a similar range before applying KNN. Common methods of feature scaling include normalization (scaling features to a range of 0 to 1) or standardization (scaling features to have zero mean and unit variance). Feature scaling allows for a fair comparison of the distances between data points and improves the overall performance of the KNN algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa610018-2a2e-48c3-82c1-61ce1cdc59aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
